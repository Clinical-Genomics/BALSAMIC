#! python
# syntax=python tabstop=4 expandtab
# coding: utf-8

import os
import hashlib

from datetime import date

from pydantic import BaseModel, ValidationError, validator, Field, AnyUrl
from pydantic.types import DirectoryPath, FilePath
from typing import Optional, List, Dict

from BALSAMIC.utils.rule import get_conda_env
from BALSAMIC.utils.rule import get_script_path

VALID_REF_FORMAT = ["fasta", "vcf","text", "gtf", "gff"]
VALID_GENOME_VER = ["hg19", "hg38"]

class ReferenceUrlsModel(BaseModel):
    """Defines a basemodel for reference urls
    
    This class handles four attributes for each reference url. Each attribute defines url, type of file, and gzip status.

    Attributes:
      url: defines the url to access file. Essentially it will be used to download file locally. It should match url_type://...
      file_type: describes file type. Accepted values are VALID_REF_FORMAT constant 
      gzip: gzip status. Binary: True or False
      genome_version: genome version matching the content of the file. Accepted values are VALID_GENOME_VER constant 

    Raises:
      ValidationError: When it can't validate values matching above attributes
      
    """
    
    url: AnyUrl
    file_type: str
    gzip: bool=True
    genome_version: str
    output_file: Optional[str]
    secret: Optional[str]
    
    @validator("file_type")
    def check_file_type(cls, value)-> str:
        assert value in VALID_REF_FORMAT, f"{value} not a valid reference file formatr."
        return value

    @validator("genome_version")
    def check_genome_ver(cls, value)-> str:
        assert value in VALID_GENOME_VER, f"{value} not a valid genome version."
        return value

    @property
    def url_type(self):
        if self.url.scheme == "gs":
            get_type = "gsutil"
        else:
            get_type = "wget"
        return get_type

    @property
    def write_md5(self):
        hash_md5 = hashlib.md5()
        with open(str(self.output_file), 'rb') as fh:
            for chunk in iter(lambda: fh.read(4096), b""):
                hash_md5.update(chunk)

        with open(str(self.output_file+".md5"), 'w') as fh:
            fh.write('{} {}\n'.format(self.output_file, hash_md5.hexdigest()))

        return hash_md5.hexdigest()

# explictly check if cluster_config dict has zero keys.
if len(cluster_config.keys()) == 0:
    cluster_config = config

current_day = date.today()

# backward compatible genome version extraction from config
if 'genome_version' in config:
    genome_ver = config['genome_version']
else:
    genome_ver = 'hg19'

# essential path reference files
basedir = os.path.join(config['output'], genome_ver)
genome_dir = os.path.join(basedir, "genome")
vcf_dir = os.path.join(basedir, "variants")
vep_dir = os.path.join(basedir, "vep")
cosmicdb_key = config['cosmic_key'] 

# intialize reference files 
reference_genome_url = ReferenceUrlsModel(
    url="gs://gatk-legacy-bundles/b37/human_g1k_v37.fasta.gz",
    file_type="fasta",
    gzip=True,
    genome_version=genome_ver,
    output_file=os.path.join(genome_dir, "human_g1k_v37.fasta"),
)
dbsnp_url = ReferenceUrlsModel(
    url="gs://gatk-legacy-bundles/b37/dbsnp_138.b37.vcf.gz",
    file_type="fasta",
    gzip=True,
    genome_version=genome_ver,
    output_file=os.path.join(vcf_dir, "dbsnp_grch37_b138.vcf"),
)
hc_vcf_1kg_url = ReferenceUrlsModel(
    url="gs://gatk-legacy-bundles/b37/1000G_phase1.snps.high_confidence.b37.vcf.gz",
    file_type="vcf",
    gzip=True,
    output_file = os.path.join(vcf_dir, "1kg_phase1_snps_high_confidence_b37.vcf"),
    genome_version=genome_ver,
)
mills_1kg_url = ReferenceUrlsModel(
    url="gs://gatk-legacy-bundles/b37/Mills_and_1000G_gold_standard.indels.b37.vcf.gz",
    file_type="vcf",
    gzip=True,
    output_file=os.path.join(vcf_dir, "mills_1kg_index.vcf"),
    genome_version=genome_ver,
)
known_indel_1kg_url = ReferenceUrlsModel(
    url="gs://gatk-legacy-bundles/b37/1000G_phase1.indels.b37.vcf.gz",
    file_type="vcf",
    gzip=True,
    output_file = os.path.join(vcf_dir, "1kg_known_indels_b37.vcf.gz"),
    genome_version=genome_ver,
)
vcf_1kg_url = ReferenceUrlsModel(
    url="ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.wgs.phase3_shapeit2_mvncall_integrated_v5b.20130502.sites.vcf.gz",
    file_type="vcf",
    gzip=True,
    output_file = os.path.join(vcf_dir, "1k_genome_wgs_p1_v3_all_sites.vcf"),
    genome_version=genome_ver,
)
cosmicdb_url = ReferenceUrlsModel(
    url="https://cancer.sanger.ac.uk/cosmic/file_download/GRCh37/cosmic/v90/VCF/CosmicCodingMuts.vcf.gz",
    file_type="vcf",
    gzip=True,
    output_file=os.path.join(vcf_dir, "cosmic_coding_muts_v89.vcf"),
    genome_version=genome_ver,
    secret=config['cosmic_key']
)
refgene_txt_url = ReferenceUrlsModel(
    url="http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/refGene.txt.gz",
    file_type="text",
    gzip=True,
    output_file=os.path.join(genome_dir, "refGene.txt"),
    genome_version=genome_ver,
)
refgene_sql_url = ReferenceUrlsModel(
    url="http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/refGene.sql",
    file_type="text",
    gzip=True,
    genome_version=genome_ver,
)
wgs_calling_url = ReferenceUrlsModel(
    url="gs://gatk-legacy-bundles/b37/wgs_calling_regions.v1.interval_list",
    file_type="text",
    gzip=False,
    output_file=os.path.join(genome_dir, "wgs_calling_regions.v1"),
    genome_version=genome_ver,
)
genome_chrom_size_url = ReferenceUrlsModel(
    url="https://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/hg19.chrom.sizes",
    file_type="text",
    gzip=False,
    output_file=os.path.join(genome_dir, "hg19.chrom.sizes"),
    genome_version=genome_ver,
)

# VCF files list for wildcards
VCF = ['dbsnp_grch37_b138.vcf', '1k_genome_wgs_p1_v3_all_sites.vcf',
       '1kg_phase1_snps_high_confidence_b37.vcf', 'mills_1kg_index.vcf', 'cosmic_coding_muts_v89.vcf']


check_md5 = os.path.join(basedir, "reference_" + str(current_day) + ".md5")


shell.prefix("set -eo pipefail; ")


def get_md5(filename):
    hash_md5 = hashlib.md5()
    with open(str(filename), 'rb') as fh:
        for chunk in iter(lambda: fh.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()


def create_md5(reference, check_md5):
    """ create a md5 file for all reference data"""
    with open(check_md5, 'w') as fh:
        for key, value in reference.items():
            if os.path.isfile(value):
                fh.write( get_md5(value) + ' ' + value + '\n')


singularity_image = config['singularity']['image']

##########################################################
# Generating Reference files for BALSAMIC pipeline
# Writing reference json file 
#
##########################################################

rule all:
    input:
        reference_genome = reference_genome_url.output_file,
        bwa_index = expand(reference_genome_url.output_file + "{ext}", ext=['.amb','.ann','.bwt','.pac','.sa']),
        refgenome_fai = reference_genome_url.output_file + ".fai",
        refgenome_dict = reference_genome_url.output_file.replace("fasta","dict"),
        refseq_bed = refgene_txt_url.output_file.replace("txt", "flat") + ".bed",
        refseq_flat = refgene_txt_url.output_file.replace("txt", "flat"),
        refgene = refgene_txt_url.output_file,
        dbsnp_vcf = dbsnp_url.output_file + ".gz",
        th_genome_vcf = vcf_1kg_url.output_file + ".gz",
        tg_high_vcf = hc_vcf_1kg_url.output_file+ ".gz",
        mills_1kg = mills_1kg_url.output_file + ".gz",
        known_indel_1kg = known_indel_1kg_url.output_file + ".gz",
        cosmic_vcf = cosmicdb_url.output_file + ".gz",
        variants_idx = expand( os.path.join(vcf_dir,"{vcf}.gz.tbi"), vcf=VCF),
        vep = directory(vep_dir),
        wgs_calling = wgs_calling_url.output_file,
        genome_chrom_size = genome_chrom_size_url.output_file 
    output:
        finished = os.path.join(basedir,"reference.finished"),
        reference_json = os.path.join(basedir, "reference.json"),
        check_md5 = check_md5
    log:
        os.path.join(basedir, "reference.json.log")
    run:
        import json

        ref_json = dict()
        ref_json['reference'] = {
            "reference_genome": input.reference_genome,
            "dbsnp": input.dbsnp_vcf,
            "1kg_snps_all": input.th_genome_vcf,
            "1kg_snps_high": input.tg_high_vcf,
            "1kg_known_indel": input.known_indel_1kg,
            "mills_1kg": input.mills_1kg,
            "cosmic": input.cosmic_vcf,
            "exon_bed": input.refseq_bed,
            "refflat": input.refseq_flat,
            "refGene": input.refgene,
            "wgs_calling_interval": input.wgs_calling,
            "genome_chrom_size": input.genome_chrom_size,
            "vep": input.vep
        }

        with open(str(output.reference_json), "w") as fh:
            json.dump(ref_json, fh, indent=4)
        
        create_md5(ref_json['reference'], output.check_md5)

        shell("date +'%Y-%M-%d T%T %:z' > {output.finished}") 


##########################################################
# Download the reference genome, variant db 
#                       - .fasta, dbsnp.vcf, 1kg.vcf, refFlat
##########################################################
reference_data = [reference_genome_url, dbsnp_url, hc_vcf_1kg_url, mills_1kg_url, known_indel_1kg_url, vcf_1kg_url,
wgs_calling_url, genome_chrom_size_url, cosmicdb_url]

rule download_reference:
    output:
        expand("{output}", output=[ref.output_file for ref in reference_data])
    run:
        import requests

        for ref in reference_data:
            print(ref.dict())
            if ref.url_type == "gsutil":
                cmd = "gsutil cp -L {}.log {} -".format(ref.output_file, ref.url)
            else:
                cmd = "wget -a {}.log -O - {}".format(ref.output_file, ref.url)

            if ref.secret:
                response = requests.get(ref.url, headers={'Authorization': 'Basic %s' % ref.secret })
                download_url = response.json()["url"]
                cmd = "curl -o - '{}'".format(download_url)
            
            if ref.gzip:
                cmd += " | gunzip "

            cmd += " > {}".format(ref.output_file)
            shell(cmd)
            ref.write_md5


            
rule download_refgene:
    params:
        refgene_txt = refgene_txt_url.url,
        refgene_sql = refgene_sql_url.url,
        refgene_sql_awk = get_script_path('refseq_sql.awk'), 
        bed_header="chrom,exonStarts,exonEnds,name,score,strand,exonCount,txStart,txEnd,name2",
        conda_env = get_conda_env(config["conda_env_yaml"], "bedtools")
    output:
        refflat = refgene_txt_url.output_file.replace("txt", "flat"),
        refgene = refgene_txt_url.output_file,
        bed = refgene_txt_url.output_file.replace("txt", "flat") + ".bed"
    log:
        refgene_sql = os.path.join(genome_dir, "refgene_sql.log"),
        refgene_txt = os.path.join(genome_dir, "refgene_txt.log")
    singularity: singularity_image
    shell:
        "source activate {params.conda_env}; " 
        "header=$(wget -a {log.refgene_sql} -O - {params.refgene_sql} | awk -f {params.refgene_sql_awk}); "
        "wget -O - {params.refgene_txt} | gunzip | sed 's/chr//g' > {output.refgene};"
        "(echo \"$header\"; wget -a {log.refgene_txt} -O - {params.refgene_txt} | gunzip;) "
        "| csvcut -t -c chrom,exonStarts,exonEnds,name,score,strand,exonCount,txStart,txEnd,name2 "
        "| csvformat -T "
        "| bedtools expand -c 2,3 "
        "| awk '$1~/chr[1-9]/ && $1!~/[_]/' | cut -c 4- | sort -k1,1 -k2,2n > {output.bed}; "
        "(wget -a {log.refgene_txt} -O - {params.refgene_txt} | gunzip) "
        "| awk -v OFS=\"\\t\" '$3!~/_/ {{ gsub(\"chr\",\"\",$3); $1=$13; print }}' "
        "| cut -f 1-11 > {output.refflat}; "
        "source deactivate; "

##########################################################
# Bgzipping and tabix the vcf files
# 
##########################################################

rule bgzip_tabix:
    input: 
        os.path.join(vcf_dir, "{vcf}")
    params:
        type = 'vcf',
        conda_env = get_conda_env(config["conda_env_yaml"], "tabix")    
    output:
        os.path.join(vcf_dir, "{vcf}.gz"),
        os.path.join(vcf_dir, "{vcf}.gz.tbi")
    log:
        os.path.join(vcf_dir, "{vcf}.gz_tbi.log")
    singularity: singularity_image
    shell:
        "source activate {params.conda_env};"
        "bgzip {input} && tabix -p {params.type} {input}.gz 2> {log};"
        "source deactivate;"


##########################################################
# Create BWA Index for reference genome
#
##########################################################

rule bwa_index:
    input:
        reference_genome_url.output_file
    params:
        conda_env = get_conda_env(config["conda_env_yaml"], "bwa")
    output:
        expand(reference_genome_url.output_file + "{ext}", ext=['.amb','.ann','.bwt','.pac','.sa'])
    log:
        reference_genome_url.output_file + ".bwa_index.log"
    singularity: singularity_image
    shell:
        "source activate {params.conda_env};"
        "bwa index -a bwtsw {input} 2> {log};"
        "source deactivate;"

##########################################################
# Create index for fasta file - .fai
# 
##########################################################

rule samtools_index_fasta:
    input:
        reference_genome_url.output_file
    params:
        conda_env = get_conda_env(config["conda_env_yaml"], "samtools")
    output:
        reference_genome_url.output_file + ".fai"
    log:
        reference_genome_url.output_file + ".faidx.log"
    singularity: singularity_image
    shell:
        "source activate {params.conda_env};"
        "samtools faidx {input} 2> {log};"
        "source deactivate;"


##########################################################
# create reference dictionary using picard
# 
##########################################################

rule picard_ref_dict:
    input:
        reference_genome_url.output_file
    params:
        conda_env = get_conda_env(config["conda_env_yaml"], "picard")
    output:
        reference_genome_url.output_file.replace("fasta","dict")
    log:
        reference_genome_url.output_file + ".ref_dict.log"
    singularity: singularity_image
    shell:
        "source activate {params.conda_env};"
        "picard CreateSequenceDictionary "
          " REFERENCE={input} " 
          " OUTPUT={output} 2> {log};"
        "source deactivate;"


##########################################################
# ENSEMBL VEP - download and install vep package, 
#                 cache coversion
##########################################################

rule vep_install:
    params:
        species = "homo_sapiens_merged",
        assembly = "GRCh37",
        plugins = "all",
        conda_env = get_conda_env(config["conda_env_yaml"], "ensembl-vep")
    output:
        directory(vep_dir)
    log:
        os.path.join(vep_dir, "vep_install_cache.log")
    singularity: singularity_image
    shell:
        "source activate {params.conda_env};"
        "vep_install --SPECIES {params.species} "
          " --AUTO cfp "
          " --ASSEMBLY {params.assembly} "
          " --CACHEDIR {output} "
          " --PLUGINS {params.plugins} "
          " --NO_HTSLIB --CONVERT --NO_UPDATE 2> {log}; "
          "source deactivate;"

