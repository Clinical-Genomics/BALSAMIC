#! python
# syntax=python tabstop=4 expandtab
# coding: utf-8

import os
import hashlib

from datetime import date

from pydantic import BaseModel, ValidationError, validator, Field, AnyUrl
from pydantic.types import DirectoryPath, FilePath
from typing import Optional, List, Dict

from BALSAMIC.utils.rule import get_conda_env
from BALSAMIC.utils.rule import get_script_path

VALID_REF_FORMAT = ["fasta", "vcf","text", "gtf", "gff"]
VALID_GENOME_VER = ["hg19", "hg38"]

class ReferenceUrlsModel(BaseModel):
    """Defines a basemodel for reference urls
    
    This class handles four attributes for each reference url. Each attribute defines url, type of file, and gzip status.

    Attributes:
      url: defines the url to access file. Essentially it will be used to download file locally. It should match url_type://...
      file_type: describes file type. Accepted values are VALID_REF_FORMAT constant 
      gzip: gzip status. Binary: True or False
      genome_version: genome version matching the content of the file. Accepted values are VALID_GENOME_VER constant 

    Raises:
      ValidationError: When it can't validate values matching above attributes
      
    """
    
    url: AnyUrl
    file_type: str
    gzip: bool=True
    genome_version: str
    output_file: Optional[str]
    
    @validator("file_type")
    def check_file_type(cls, value)-> str:
        assert value in VALID_REF_FORMAT, f"{value} not a valid reference file formatr."
        return value

    @validator("genome_version")
    def check_genome_ver(cls, value)-> str:
        assert value in VALID_GENOME_VER, f"{value} not a valid genome version."
        return value

    @property
    def url_type(self):
        if self.url.scheme == "gs":
            get_type = "gsutil"
        else:
            get_type = "wget"
        return get_type

# explictly check if cluster_config dict has zero keys.
if len(cluster_config.keys()) == 0:
    cluster_config = config

current_day = date.today()

# backward compatible genome version extraction from config
if 'genome_version' in config:
    genome_ver = config['genome_version']
else:
    genome_ver = 'hg19'

# essential path reference files
basedir = os.path.join(config['output'], genome_ver)
genome_dir = os.path.join(basedir, "genome")
vcf_dir = os.path.join(basedir, "variants")
vep_dir = os.path.join(basedir, "vep")
cosmicdb_key = config['cosmic_key'] 

# intialize reference files 
reference_genome_url = ReferenceUrlsModel(
    url="gs://gatk-legacy-bundles/b37/human_g1k_v37.fasta.gz",
    file_type="fasta",
    gzip=True,
    genome_version=genome_ver,
    output_file=os.path.join(genome_dir, "human_g1k_v37.fasta"),
)
dbsnp_url = ReferenceUrlsModel(
    url="gs://gatk-legacy-bundles/b37/dbsnp_138.b37.vcf.gz",
    file_type="fasta",
    gzip=True,
    genome_version=genome_ver,
    output_file=os.path.join(vcf_dir, "dbsnp_grch37_b138.vcf"),
)
hc_vcf_1kg_url = ReferenceUrlsModel(
    url="gs://gatk-legacy-bundles/b37/1000G_phase1.snps.high_confidence.b37.vcf.gz",
    file_type="vcf",
    gzip=True,
    output_file = os.path.join(vcf_dir, "1kg_phase1_snps_high_confidence_b37.vcf"),
    genome_version=genome_ver,
)
mills_1kg_url = ReferenceUrlsModel(
    url="gs://gatk-legacy-bundles/b37/Mills_and_1000G_gold_standard.indels.b37.vcf.gz",
    file_type="vcf",
    gzip=True,
    output_file=os.path.join(vcf_dir, "mills_1kg_index.vcf"),
    genome_version=genome_ver,
)
known_indel_1kg_url = ReferenceUrlsModel(
    url="gs://gatk-legacy-bundles/b37/1000G_phase1.indels.b37.vcf.gz",
    file_type="vcf",
    gzip=True,
    output_file = os.path.join(vcf_dir, "1kg_known_indels_b37.vcf.gz"),
    genome_version=genome_ver,
)
vcf_1kg_url = ReferenceUrlsModel(
    url="ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.wgs.phase3_shapeit2_mvncall_integrated_v5b.20130502.sites.vcf.gz",
    file_type="vcf",
    gzip=True,
    output_file = os.path.join(vcf_dir, "1k_genome_wgs_p1_v3_all_sites.vcf"),
    genome_version=genome_ver,
)
cosmicdb_url = ReferenceUrlsModel(
    url="https://cancer.sanger.ac.uk/cosmic/file_download/GRCh37/cosmic/v90/VCF/CosmicCodingMuts.vcf.gz",
    file_type="vcf",
    gzip=True,
    genome_version=genome_ver,
)
refgene_txt_url = ReferenceUrlsModel(
    url="http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/refGene.txt.gz",
    file_type="text",
    gzip=True,
    genome_version=genome_ver,
)
refgene_sql_url = ReferenceUrlsModel(
    url="http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/refGene.sql",
    file_type="text",
    gzip=True,
    genome_version=genome_ver,
)
wgs_calling_url = ReferenceUrlsModel(
    url="gs://gatk-legacy-bundles/b37/wgs_calling_regions.v1.interval_list",
    file_type="text",
    gzip=True,
    genome_version=genome_ver,
)
genome_chrom_size_url = ReferenceUrlsModel(
    url="https://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/hg19.chrom.sizes",
    file_type="text",
    gzip=True,
    genome_version=genome_ver,
)

# VCF files list for wildcards
VCF = ['dbsnp_grch37_b138.vcf', '1k_genome_wgs_p1_v3_all_sites.vcf',
       '1kg_phase1_snps_high_confidence_b37.vcf', 'mills_1kg_index.vcf', 'cosmic_coding_muts_v89.vcf']


# OUTPUT FILE NAMES
reference_json = os.path.join(basedir, "reference.json")

refgene = os.path.join(genome_dir, "refGene.txt")
refseq_flat = os.path.join(genome_dir, "refseq.flat")
refseq_bed = refseq_flat + ".bed"
wgs_calling_interval =  os.path.join(genome_dir, "wgs_calling_regions.v1")
genome_dict = os.path.join(genome_dir, "human_g1k_v37_decoy.dict")
genome_chrom_size = os.path.join(genome_dir, "hg19.chrom.sizes")

cosmicdb = os.path.join(vcf_dir, "cosmic_coding_muts_v89.vcf")
check_md5 = os.path.join(basedir, "reference_" + str(current_day) + ".md5")

# list of reference download link and output file name
ref_list_gzip = [(reference_genome_url.url, reference_genome_url.output_file, reference_genome_url.url_type),
                (dbsnp_url.url, dbsnp_url.output_file, dbsnp_url.url_type),
                (hc_vcf_1kg_url.url, hc_vcf_1kg_url.output_file, hc_vcf_1kg_url.url_type),
                (mills_1kg_url.url, mills_1kg_url.output_file, mills_1kg_url.url_type),
                (known_indel_1kg_url.url, known_indel_1kg_url.output_file, known_indel_1kg_url.url_type),
                (vcf_1kg_url.url, vcf_1kg_url.output_file, vcf_1kg_url.url_type)]

ref_list_flat = [(wgs_calling_url.url, wgs_calling_interval, wgs_calling_url.url_type),
                 (genome_chrom_size_url.url, genome_chrom_size, genome_chrom_size_url.url_type)] 

shell.prefix("set -eo pipefail; ")


def get_md5(filename):
    hash_md5 = hashlib.md5()
    with open(str(filename), 'rb') as fh:
        for chunk in iter(lambda: fh.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()


def create_md5(reference, check_md5):
    """ create a md5 file for all reference data"""
    with open(check_md5, 'w') as fh:
        for key, value in reference.items():
            if os.path.isfile(value):
                fh.write( get_md5(value) + ' ' + value + '\n')


singularity_image = config['singularity']['image']

##########################################################
# Generating Reference files for BALSAMIC pipeline
# Writing reference json file 
#
##########################################################

rule all:
    input:
        reference_genome = reference_genome_url.output_file,
        bwa_index = expand(reference_genome_url.output_file + "{ext}", ext=['.amb','.ann','.bwt','.pac','.sa']),
        refgenome_fai = reference_genome_url.output_file + ".fai",
        refgenome_dict = genome_dict,
        refseq_bed = refseq_bed,
        refseq_flat = refseq_flat,
        refgene = refgene,
        dbsnp_vcf = dbsnp_url.output_file + ".gz",
        th_genome_vcf = vcf_1kg_url.output_file + ".gz",
        tg_high_vcf = hc_vcf_1kg_url.output_file+ ".gz",
        mills_1kg = mills_1kg_url.output_file + ".gz",
        known_indel_1kg = known_indel_1kg_url.output_file + ".gz",
        cosmic_vcf = cosmicdb + ".gz",
        variants_idx = expand( os.path.join(vcf_dir,"{vcf}.gz.tbi"), vcf=VCF),
        vep = vep_dir,
        wgs_calling = wgs_calling_interval,
        genome_chrom_size = genome_chrom_size
    output:
        finished = os.path.join(basedir,"reference.finished"),
        reference_json = reference_json,
        check_md5 = check_md5
    log:
        reference_json + ".log"
    run:
        import json

        ref_json = dict()
        ref_json['reference'] = {
            "reference_genome": input.reference_genome,
            "dbsnp": input.dbsnp_vcf,
            "1kg_snps_all": input.th_genome_vcf,
            "1kg_snps_high": input.tg_high_vcf,
            "1kg_known_indel": input.known_indel_1kg,
            "mills_1kg": input.mills_1kg,
            "cosmic": input.cosmic_vcf,
            "exon_bed": input.refseq_bed,
            "refflat": input.refseq_flat,
            "refGene": input.refgene,
            "wgs_calling_interval": input.wgs_calling,
            "genome_chrom_size": input.genome_chrom_size,
            "vep": input.vep
        }

        with open(str(output.reference_json), "w") as fh:
            json.dump(ref_json, fh, indent=4)
        
        create_md5(ref_json['reference'], output.check_md5)

        shell("date +'%Y-%M-%d T%T %:z' > {output.finished}") 


##########################################################
# Download the reference genome, variant db 
#                       - .fasta, dbsnp.vcf, 1kg.vcf, refFlat
##########################################################

rule download_reference:
    output:
        expand("{output}", output=[ref[1] for ref in ref_list_gzip + ref_list_flat])
    run:
        for ref in ref_list_gzip:
            if ref[2] == "gsutil":
                shell("gsutil cp -L {ref[1]}.log {ref[0]} - | gunzip > {ref[1]}") 
            else:
                shell("wget -a {ref[1]}.log -O - {ref[0]} | gunzip > {ref[1]}")
        for ref in ref_list_flat:
            if ref[2] == "gsutil":
                shell("gsutil cp -L {ref[1]}.log {ref[0]} {ref[1]}") 
            else:
                shell("wget -a {ref[1]}.log -O {ref[1]} {ref[0]}")

rule download_refgene:
    params:
        refgene_txt = refgene_txt_url.url,
        refgene_sql = refgene_sql_url.url,
        refgene_sql_awk = get_script_path('refseq_sql.awk'), 
        bed_header="chrom,exonStarts,exonEnds,name,score,strand,exonCount,txStart,txEnd,name2",
        conda_env = get_conda_env(config["conda_env_yaml"], "bedtools")
    output:
        refflat = refseq_flat,
        refgene = refgene,
        bed = refseq_bed
    log:
        refgene_sql = os.path.join(genome_dir, "refgene_sql.log"),
        refgene_txt = os.path.join(genome_dir, "refgene_txt.log")
    singularity: singularity_image
    shell:
        "source activate {params.conda_env}; " 
        "header=$(wget -a {log.refgene_sql} -O - {params.refgene_sql} | awk -f {params.refgene_sql_awk}); "
        "wget -O - {params.refgene_txt} | gunzip | sed 's/chr//g' > {output.refgene};"
        "(echo \"$header\"; wget -a {log.refgene_txt} -O - {params.refgene_txt} | gunzip;) "
        "| csvcut -t -c chrom,exonStarts,exonEnds,name,score,strand,exonCount,txStart,txEnd,name2 "
        "| csvformat -T "
        "| bedtools expand -c 2,3 "
        "| awk '$1~/chr[1-9]/ && $1!~/[_]/' | cut -c 4- | sort -k1,1 -k2,2n > {output.bed}; "
        "(wget -a {log.refgene_txt} -O - {params.refgene_txt} | gunzip) "
        "| awk -v OFS=\"\\t\" '$3!~/_/ {{ gsub(\"chr\",\"\",$3); $1=$13; print }}' "
        "| cut -f 1-11 > {output.refflat}; "
        "source deactivate; "

##########################################################
# Download cosmic data using basic authentication 
#
##########################################################

rule download_cosmicdb:
    params:
        cosmic_db = cosmicdb_url.url,
        cosmicdb_key = cosmicdb_key,
        conda_env = get_conda_env(config["conda_env_yaml"], "bwa")
    output:
        cosmicdb
    run:
        import requests

        ## request the download url link
        response = requests.get(params.cosmic_db, headers={'Authorization': 'Basic %s' % params.cosmicdb_key })
        download_url = response.json()["url"]

        ## request the download file 
        vcf = requests.get(download_url)

        ## write the file 
        with open(str(output)+'.gz', "wb") as fh:
          fh.write(vcf.content)

        ## unzip the file for indexing
        shell("gunzip {output}.gz;")


##########################################################
# Bgzipping and tabix the vcf files
# 
##########################################################

rule bgzip_tabix:
    input: 
        os.path.join(vcf_dir, "{vcf}")
    params:
        type = 'vcf',
        conda_env = get_conda_env(config["conda_env_yaml"], "tabix")    
    output:
        os.path.join(vcf_dir, "{vcf}.gz"),
        os.path.join(vcf_dir, "{vcf}.gz.tbi")
    log:
        os.path.join(vcf_dir, "{vcf}.gz_tbi.log")
    singularity: singularity_image
    shell:
        "source activate {params.conda_env};"
        "bgzip {input} && tabix -p {params.type} {input}.gz 2> {log};"
        "source deactivate;"


##########################################################
# Create BWA Index for reference genome
#
##########################################################

rule bwa_index:
    input:
        reference_genome_url.output_file
    params:
        conda_env = get_conda_env(config["conda_env_yaml"], "bwa")
    output:
        expand(reference_genome_url.output_file + "{ext}", ext=['.amb','.ann','.bwt','.pac','.sa'])
    log:
        reference_genome_url.output_file + ".bwa_index.log"
    singularity: singularity_image
    shell:
        "source activate {params.conda_env};"
        "bwa index -a bwtsw {input} 2> {log};"
        "source deactivate;"

##########################################################
# Create index for fasta file - .fai
# 
##########################################################

rule samtools_index_fasta:
    input:
        reference_genome_url.output_file
    params:
        conda_env = get_conda_env(config["conda_env_yaml"], "samtools")
    output:
        reference_genome_url.output_file + ".fai"
    log:
        reference_genome_url.output_file + ".faidx.log"
    singularity: singularity_image
    shell:
        "source activate {params.conda_env};"
        "samtools faidx {input} 2> {log};"
        "source deactivate;"


##########################################################
# create reference dictionary using picard
# 
##########################################################

rule picard_ref_dict:
    input:
        reference_genome_url.output_file
    params:
        conda_env = get_conda_env(config["conda_env_yaml"], "picard")
    output:
        genome_dict
    log:
        reference_genome_url.output_file + ".ref_dict.log"
    singularity: singularity_image
    shell:
        "source activate {params.conda_env};"
        "picard CreateSequenceDictionary "
          " REFERENCE={input} " 
          " OUTPUT={output} 2> {log};"
        "source deactivate;"


##########################################################
# ENSEMBL VEP - download and install vep package, 
#                 cache coversion
##########################################################

rule vep_install:
    params:
        species = "homo_sapiens_merged",
        assembly = "GRCh37",
        plugins = "all",
        conda_env = get_conda_env(config["conda_env_yaml"], "ensembl-vep")
    output:
        directory(vep_dir)
    log:
        os.path.join(vep_dir, "vep_install_cache.log")
    singularity: singularity_image
    shell:
        "source activate {params.conda_env};"
        "vep_install --SPECIES {params.species} "
          " --AUTO cfp "
          " --ASSEMBLY {params.assembly} "
          " --CACHEDIR {output} "
          " --PLUGINS {params.plugins} "
          " --NO_HTSLIB --CONVERT --NO_UPDATE 2> {log}; "
          "source deactivate;"

