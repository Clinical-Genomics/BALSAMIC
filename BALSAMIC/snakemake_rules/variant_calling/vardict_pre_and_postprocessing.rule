

rule bedtools_splitbed_by_chrom:
    input:
        bed = config["panel"]["capture_kit"],
        chrom = config["reference"]["genome_chrom_size"],
        bam = config_model.get_final_bam_name(bam_dir = bam_dir, sample_name = tumor_sample)
    output:
        bed = expand(vcf_dir + "split_bed/" + "{chrom}." + capture_kit, chrom=chromlist)
    benchmark:
        Path(benchmark_dir, 'bedtools_splitbed_by_chrom.tsv').as_posix()
    singularity:
        Path(singularity_image, config["bioinfo_tools"].get("bedtools") + ".sif").as_posix()
    params:
        tmpdir = tempfile.mkdtemp(prefix=tmp_dir),
        split_bed_dir = vcf_dir + "split_bed/",
        origin_bed = capture_kit,
    message:
        ("Splitting the panel bed per chromosome, flanking regions by 100bp and merging into single VCF using bedtools")
    shell:
        """
mkdir -p {params.tmpdir};
export TMPDIR={params.tmpdir};

chromlist=`cut -f 1 {input.bed} | sort -u`;
sed 's/^chr//g;/_/d' {input.chrom} | sort -k1,1 > {params.split_bed_dir}hg19.chrom.sizes;

for c in $chromlist; do \
awk -v C=$c '$1==C' {input.bed} \
| bedtools slop -b 100 -i - -g {params.split_bed_dir}hg19.chrom.sizes \
| sort -k1,1 -k2,2n \
| bedtools merge > {params.split_bed_dir}$c.{params.origin_bed}; done;

unset chromlist; 
readlink -f {input.bam};
        """

rule vardict_merge:
    input:
        expand(vcf_dir + "vardict/split_vcf/{chrom}_vardict.vcf.gz", chrom=chromlist)
    output:
        vcf_vardict = vcf_dir + "vardict/merged.vardict.vcf"
    params:
        tmpdir = tempfile.mkdtemp(prefix=tmp_dir),
        sort_vcf = get_script_path("sort_vcf.awk"),
        case_name = config["analysis"]["case_id"],
    benchmark:
        Path(benchmark_dir,'vardict_merge_' + config["analysis"]["case_id"] + ".tsv").as_posix()
    singularity:
        Path(singularity_image, config["bioinfo_tools"].get("bcftools") + ".sif").as_posix()
    threads:
        get_threads(cluster_config,"vardict_merge")
    message:
        ("Merging multiple VCFs from vardict into single VCF using bcftools for {params.case_name}")
    shell:
        """
mkdir -p {params.tmpdir};
export TMPDIR={params.tmpdir};

bcftools concat {input} > {output.vcf_vardict} ;

rm -rf {params.tmpdir} ;
    """

rule vardict_sort:
    input:
        vcf = vcf_dir + "vardict/merged.vardict.vcf"
    output:
        vcf_sorted = vcf_dir + "vardict/merged_sorted.vardict.vcf"
    params:
        tmpdir=tempfile.mkdtemp(prefix=tmp_dir),
        sort_vcf=get_script_path("sort_vcf.awk"),
        case_name=config["analysis"]["case_id"],
    benchmark:
        Path(benchmark_dir, 'vardict_sort_' + config["analysis"]["case_id"] + ".tsv").as_posix()
    message:
        ("Sorting merged vardict files with awk {params.case_name}")
    shell:
        """
mkdir -p {params.tmpdir};
export TMPDIR={params.tmpdir};

awk -f {params.sort_vcf} {input.vcf} > {output.vcf_sorted}

rm -rf {params.tmpdir} ;
    """

rule gatk_update_vcf_sequence_dictionary:
    input:
        ref = config["reference"]["reference_genome"],
        vcf = vcf_dir + "SNV.somatic." + config["analysis"]["case_id"] + ".preprocess_vardict.vcf.gz",
    output:
        vcf = vcf_dir + "SNV.somatic." + config["analysis"]["case_id"] + ".vardict.vcf.gz",
    params:
        tmpdir=tempfile.mkdtemp(prefix=tmp_dir),
    benchmark:
        Path(benchmark_dir,"gatk_update_vcf_sequence_dictionary" + config["analysis"]["case_id"] + ".tsv").as_posix()
    singularity:
        Path(singularity_image,config["bioinfo_tools"].get("gatk") + ".sif").as_posix()
    threads:
        get_threads(cluster_config,"gatk_collectreadcounts")
    message:
        "Running GATK UpdateVCFSequenceDictionary on VarDict VCF."
    shell:
        """
export TMPDIR={params.tmpdir};  

ref=$(echo {input.ref} | sed 's/.fasta/.dict/g') ;

gatk UpdateVCFSequenceDictionary -V {input.vcf} --source-dictionary $ref --replace --output {output.vcf} ;
      
rm -rf {params.tmpdir}
        """