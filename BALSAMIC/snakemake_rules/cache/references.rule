# vim: syntax=python tabstop=4 expandtab
# coding: utf-8


rule download_references:
    """Download Balsamic reference files."""
    output:
        reference_path=f"{config['references_dir']}/{{reference_file}}",
    params:
        reference=lambda wildcards: cache_config.get_reference_by_path(
            Path(config["references_dir"], wildcards.reference_file).as_posix()
        ),
    threads:
        get_threads(cluster_config, "download_references")
    message:
        "Downloading reference file {output.reference_path}"
    log:
        "logs/{reference_file}.log",
    shell:
        """
        export TMPDIR=$(dirname "{output.reference_path}")
        if [[ -n "{params.reference.secret}" ]] && [[ "{params.reference.secret}" != "None" ]]; then
            response=$(curl -s -H "Authorization: Basic {params.reference.secret}" "{params.reference.url}")
            download_url=$(echo $response | grep -o 'https://[^"]*')
            cmd="curl '$download_url' -o -"
        elif [[ "{params.reference.url}" == gs://* ]]; then
            cmd="gsutil cp '{params.reference.url}' -"
        else
            cmd="wget '{params.reference.url}' -O -"
        fi 
        if [[ "{params.reference.gzip}" == "True" ]]; then
            cmd+=" | gunzip"
        fi
        eval "$cmd > '{output.reference_path}'" &> "{log}"
        """

# ##########################################################
# # Preprocess refseq file by fetching relevant columns and
# # standardize the chr column
# ##########################################################
#
#
# rule prepare_refgene:
#     input:
#         cache_config.container_paths,
#         refgene_txt=refgene_txt_url.get_output_file,
#         refgene_sql=refgene_sql_url.get_output_file,
#         accessible_regions=access_regions_url.get_output_file,
#     params:
#         refgene_sql_awk=get_script_path("refseq_sql.awk"),
#     output:
#         refflat=refgene_txt_url.get_output_file.replace("txt", "flat"),
#         bed=refgene_txt_url.get_output_file.replace("txt", "flat") + ".bed",
#     log:
#         refgene_sql=Path(reference_dir, "genome", "refgene_sql.log"),
#         refgene_txt=Path(reference_dir, "genome", "refgene_txt.log"),
#     singularity:
#         Path(
#             singularity_image_path, config["bioinfo_tools"].get("bedtools") + ".sif"
#         ).as_posix()
#     shell:
#         """
# header=$(awk -f {params.refgene_sql_awk} {input.refgene_sql});
# (echo \"$header\"; cat {input.refgene_txt};) \
# | csvcut -t -c chrom,exonStarts,exonEnds,name,score,strand,exonCount,txStart,txEnd,name2 \
# | csvformat -T \
# | bedtools expand -c 2,3 \
# | awk '$1~/chr[1-9]/ && $1!~/[_]/' | cut -c 4- | sort -k1,1 -k2,2n > {output.bed};
#
# awk -v OFS=\"\\t\" '$3!~/_/ {{ gsub(\"chr\",\"\",$3); $1=$13; print }}' {input.refgene_txt} \
# | cut -f 1-11 > {output.refflat};
# sed -i 's/chr//g' {input.refgene_txt};
# sed -i 's/chr//g' {input.accessible_regions};
#         """
#
#
# ##########################################################
# # bgzip and tabix the vcf files that are vcf
# ##########################################################
#
#
# rule bgzip_tabix:
#     input:
#         singularity_img=cache_config.container_paths,
#         vcf=Path(vcf_dir, "{vcf}.vcf"),
#     params:
#         type="vcf",
#     output:
#         Path(vcf_dir, "{vcf}.vcf.gz"),
#         Path(vcf_dir, "{vcf}.vcf.gz.tbi"),
#     log:
#         Path(vcf_dir, "{vcf}.vcf.gz_tbi.log"),
#     singularity:
#         Path(
#             singularity_image_path, config["bioinfo_tools"].get("tabix") + ".sif"
#         ).as_posix()
#     shell:
#         """
# bgzip {input.vcf} && tabix -p {params.type} {input.vcf}.gz 2> {log};
#         """
#
#
# ##########################################################
# # Create BWA Index for reference genome
# ##########################################################
#
#
# rule bwa_index:
#     input:
#         singularity_img=cache_config.container_paths,
#         reference_genome=reference_genome_url.get_output_file,
#     output:
#         expand(
#             reference_genome_url.get_output_file + "{ext}",
#             ext=[".amb", ".ann", ".bwt", ".pac", ".sa"],
#         ),
#     log:
#         reference_genome_url.get_output_file + ".bwa_index.log",
#     singularity:
#         Path(
#             singularity_image_path, config["bioinfo_tools"].get("bwa") + ".sif"
#         ).as_posix()
#     shell:
#         """
# bwa index -a bwtsw {input.reference_genome} 2> {log};
#         """
#
#
# ##########################################################
# # Create index for fasta file - .fai
# ##########################################################
#
#
# rule samtools_index_fasta:
#     input:
#         singularity_img=cache_config.container_paths,
#         reference_genome=reference_genome_url.get_output_file,
#     output:
#         reference_genome_url.get_output_file + ".fai",
#     log:
#         reference_genome_url.get_output_file + ".faidx.log",
#     singularity:
#         Path(
#             singularity_image_path, config["bioinfo_tools"].get("samtools") + ".sif"
#         ).as_posix()
#     shell:
#         """
# samtools faidx {input.reference_genome} 2> {log};
#         """
#
#
# ##########################################################
# # create reference dictionary using picard
# ##########################################################
#
#
# rule picard_ref_dict:
#     input:
#         singularity_img=cache_config.container_paths,
#         reference_genome=reference_genome_url.get_output_file,
#     output:
#         reference_genome_url.get_output_file.replace("fasta", "dict"),
#     log:
#         reference_genome_url.get_output_file + ".ref_dict.log",
#     singularity:
#         Path(
#             singularity_image_path, config["bioinfo_tools"].get("picard") + ".sif"
#         ).as_posix()
#     shell:
#         """
# picard CreateSequenceDictionary REFERENCE={input.reference_genome} OUTPUT={output} 2> {log};
#         """
#
#
# ##########################################################
# # ENSEMBL VEP - download and install vep package,
# #                 cache conversion
# ##########################################################
#
#
# rule vep_install:
#     input:
#         singularity_img=cache_config.container_paths,
#     params:
#         species="homo_sapiens_merged",
#         assembly="GRCh37" if genome_version == "hg19" else "GRCh38",
#         plugins="all",
#     output:
#         directory(vep_dir),
#     log:
#         Path(vep_dir, "vep_install_cache.log"),
#     singularity:
#         Path(
#             singularity_image_path, config["bioinfo_tools"].get("ensembl-vep") + ".sif"
#         ).as_posix()
#     shell:
#         """
# vep_install --SPECIES {params.species} \
# --AUTO cfp \
# --ASSEMBLY {params.assembly} \
# --CACHEDIR {output} \
# --PLUGINS {params.plugins} \
# --NO_HTSLIB --CONVERT --NO_UPDATE 2> {log};
#         """
#
#
# ##########################################################
# # Remove chr from delly exclusion
# ##########################################################
#
#
# rule prepare_delly_exclusion:
#     input:
#         singularity_img=cache_config.container_paths,
#         delly_exclusion=delly_exclusion_url.get_output_file,
#     output:
#         delly_exclusion_converted=delly_exclusion_url.get_output_file.replace(
#             ".tsv", "_converted.tsv"
#         ),
#     log:
#         Path(reference_dir, "genome", "delly_exclusion.log"),
#     singularity:
#         Path(
#             singularity_image_path, config["bioinfo_tools"].get("delly") + ".sif"
#         ).as_posix()
#     shell:
#         """
# sed 's/chr//g' {input.delly_exclusion} > {output.delly_exclusion_converted} 2> {log}
#         """
