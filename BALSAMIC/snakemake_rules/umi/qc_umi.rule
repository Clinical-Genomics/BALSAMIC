# vim: syntax=python tabstop=4 expandtab
# coding: utf-8

## UmiAwareMarkDuplicatesWithMateCigar - umimetrics
rule picard_umiaware:
    input:
        bam = umi_dir + "{case_name}.{step}.umi.bam"
    output:
        bam = umi_qc_dir + "{case_name}.{step}.picard.umiaware.bam",
        duplicates = umi_qc_dir + "{case_name}.{step}.umi.duplicatemetrics",
        umimetrics = umi_qc_dir + "{case_name}.{step}.umi.metrics"
    benchmark:
        Path(benchmark_dir + "picard_umiaware_{case_name}_{step}.tsv").as_posix()
    singularity: 
        singularity_image
    params:
        conda = get_conda_env(config["conda_env_yaml"],"picard"),
        sample_id = "{case_name}_{step}"
    threads:
        get_threads(cluster_config, "picard_umiaware")
    message:
        "Picard Umiaware mark dups for sample {params.sample_id}"
    shell:
        """
source activate {params.conda};

picard UmiAwareMarkDuplicatesWithMateCigar \
I={input.bam} \
O={output.bam} \
M={output.duplicates} \
UMI_METRICS={output.umimetrics};
        """

## CollectHSmetrics - median target coverage-required
rule picard_collecthsmetrics_umi:
    input:
        fadict = (config["reference"]["reference_genome"]).replace(".fasta",".dict"),
        bed = config["panel"]["capture_kit"],
        bam = umi_dir + "{case_name}.{step}.umi.bam",
        fa = config["reference"]["reference_genome"]
    output:
        mrkdup = umi_qc_dir + "{case_name}.{step}.umi.collect_hsmetric"
    benchmark:
        Path(benchmark_dir + "picard_collecthsmetrics_umi_{case_name}_{step}.tsv").as_posix()
    singularity:
        singularity_image
    params:
        conda = get_conda_env(config["conda_env_yaml"],"picard"),
        baitsetname = os.path.basename(config["panel"]["capture_kit"]),
        sample_id = "{case_name}_{step}"
    threads: 
        get_threads(cluster_config, "CollectHsMetrics")
    message:
        "Collect HSmetrics using Picardtools for {params.sample_id}"
    shell:
        """
source activate {params.conda};

picard BedToIntervalList \
I={input.bed} \
O={input.bam}.picard.bedintervals \
SD={input.fadict};

picard CollectHsMetrics \
BI={input.bam}.picard.bedintervals \
TI={input.bam}.picard.bedintervals \
I={input.bam} \
O={output.mrkdup};
        """

## SUM(Reads in each family)/ the number of families after correction, collapsing on supporting reads.
rule samtools_view_calculatemeanfamilydepth_umi:
    input:
        bam = umi_dir + "{case_name}.{step}.umi.bam"
    output:
        temp_fl = temp (umi_qc_dir + "{case_name}.{step}.umi.temp.fl"),
        totalsum = umi_qc_dir + "{case_name}.{step}.umi.mean_family_depth"
    benchmark:
        Path(benchmark_dir + "samtools_view_calculatemeanfamilydepth_umi_{case_name}_{step}.tsv").as_posix()
    singularity:
        singularity_image
    params:
        conda = get_conda_env(config["conda_env_yaml"],"samtools"),
        sample_id = "{case_name}_{step}"
    threads:
        get_threads(cluster_config, "samtools_view_calculatemeanfamilydepth_umi")
    message:
        "Calculating mean family depth using samtools and awk for {params.sample_id}"
    shell:
        """
source activate {params.conda};

samtools view -@ {threads} {input.bam} | \
grep 'RX:Z:' | \
sed 's/.*RX:Z:\\([ACGT-].*\\).*/\\1/' | \
cut -f1 | \
grep -v 'N' | \
sort | uniq -c | \
sed -e 's/  */\\t/g' | \
cut -f2,3 > {output.temp_fl};

awk -F'\\t' \
'{{sum+=$1;}} \
END{{printf(\"{params.sample_id}_meandepth: \"sum/NR)}}' \
{output.temp_fl} > \
{output.totalsum}
        """

## Noise AF = TotalAF_with_UMI/TotalAF_without_UMI - considering TNscope values
rule bcftools_query_calculatenoiseAF_umi:
    input:
        umiextract = vcf_dir + "SNV.somatic.{case_name}.consensusaligned.TNscope.umi.vcf.gz",
        consensuscall = vcf_dir + "SNV.somatic.{case_name}.consensusfiltered.TNscope.umi.vcf.gz"
    output:
        umiextractAF = temp (umi_qc_dir + "{case_name}.TNscope.umi.consensusaligned.AF.txt"),
        consensuscallAF = temp (umi_qc_dir + "{case_name}.TNscope.umi.consensusfiltered.AF.txt"),
        noiseAF = umi_qc_dir + "{case_name}.TNscope.umi.noiseAF"
    benchmark:
        Path(benchmark_dir + "bcftools_query_calculatenoiseAF_umi_{case_name}.tsv").as_posix()
    singularity:
        singularity_image
    params:
        conda = get_conda_env(config["conda_env_yaml"],"bcftools"),
        sample_id = "{case_name}"
    threads:
        get_threads(cluster_config, "bcftools_query_calculatenoiseAF_umi")
    message: 
        "Calculate noise AF values for {params.sample_id}"
    shell:
        """
source activate {params.conda};

bcftools query \
-f \"%CHROM:%POS:%REF->%ALT\\t[%AF]\\n\" \
{input.umiextract} > \
{output.umiextractAF};

bcftools query \
-f \"%CHROM:%POS:%REF->%ALT\\t[%AF]\\n\" \
{input.consensuscall} > \
{output.consensuscallAF};

awk '{{if (FILENAME~/umialign.AF.txt/) \
{{sum1+=$2}} \
else {{sum2+=$2}}; \
if (FNR == 1 && NR != 1) {{f1_nr = NR-1}}}} \
END {{print(\"NoiseAF: \"(sum1/f1_nr)/(sum2/(NR-f1_nr)))}}' \
{output.umiextractAF} \
{output.consensuscallAF} > \
{output.noiseAF}
        """

# Plot the TNscope calculated AFs before and after consensuscall
rule seaborn_densityplot_umi:
    input:
        table1 = umi_qc_dir + "{case_name}.TNscope.umi.consensusaligned.AF.txt",
        table2 = umi_qc_dir + "{case_name}.TNscope.umi.consensusfiltered.AF.txt"
    output:
        AF_plot = umi_qc_dir  + "{case_name}.TNscope.umi.AFplot.pdf"
    benchmark:
        Path(benchmark_dir + "seaborn_densityplot_umi_{case_name}.tsv").as_posix()
    params:
        sample_id = "{case_name}"
    threads:
        get_threads(cluster_config, "seaborn_densityplot_umi")
    message:
        "Density AF plots for sample {params.sample_id}"
    run:
        get_densityplot(input.table1, input.table2, "consensusaligned", "consensuscallfiltered", output.AF_plot)
