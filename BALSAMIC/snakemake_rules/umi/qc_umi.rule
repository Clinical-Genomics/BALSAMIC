# vim: syntax=python tabstop=4 expandtab
# coding: utf-8


from BALSAMIC.utils.rule import get_conda_env
from BALSAMIC.utils.rule import get_threads

## UmiAwareMarkDuplicatesWithMateCigar - umimetrics
rule picard_umiaware:
    input: 
        umi_dir + '{sample}.umialign.bam',
        umi_dir + '{sample}.consensusalign.bam'
    output: 
        bam = [qc_dir + '{sample}.umialign.picard.umiaware.bam', qc_dir + '{sample}.consensus.picard.umiaware.bam'],
        duplicates = [qc_dir + '{sample}.umialign.duplicatemetrics', qc_dir + '{sample}.consensus.duplicatemetrics'],
        umimetrics = [qc_dir + '{sample}.umialign.umimetrics', qc_dir + '{sample}.consensus.umimetrics'] 
    params:
        conda = get_conda_env(config["conda_env_yaml"],"picard"), 
        sample_id = '{sample}'
    singularity: singularity_image
    log: 
        log_dir + '{sample}.umiaware.log'
    benchmark: 
        benchmark_dir + '{sample}.umiaware.tsv'
    message: 
        "Picard Umiaware mark dups for sample {params.sample_id}"
    threads: 
        get_threads(cluster_config, 'picard_umiaware')
    shell:
        "source activate {params.conda}\n"
        "picard UmiAwareMarkDuplicatesWithMateCigar I={input[0]} O={output.bam[0]} M={output.duplicates[0]} UMI_METRICS={output.umimetrics[0]}\n"
        "picard UmiAwareMarkDuplicatesWithMateCigar I={input[1]} O={output.bam[1]} M={output.duplicates[1]} UMI_METRICS={output.umimetrics[1]} "


## CollectHSmetrics - median target coverage-required
rule CollectHsMetrics:
  input:
    fadict = (config["reference"]["reference_genome"]).replace(".fasta",".dict"),
    bed = config["panel"]["capture_kit"],
    bam = umi_dir + '{sample}.consensusalign.bam',
    fa = config["reference"]["reference_genome"],
  output:
    mrkdup = qc_dir + '{sample}.hsmetric'
  params:
    conda = get_conda_env(config["conda_env_yaml"],"picard"),
    baitsetname = os.path.basename(config["panel"]["capture_kit"])
  singularity: singularity_image
  benchmark:
    benchmark_dir + "CollectHsMetrics_" + "{sample}.collect_hsmetrics.tsv"
  shell:
    "source activate {params.conda};"
    "picard "
      "BedToIntervalList "
      "I={input.bed} "
      "O={input.bam}.picard.bedintervals "
      "SD={input.fadict}; "
    "picard "
      "CollectHsMetrics "
      "BI={input.bam}.picard.bedintervals "
      "TI={input.bam}.picard.bedintervals "
      "I={input.bam} "
      "O={output.mrkdup} "

## SUM(Reads in each family)/ the number of families after correction, collapsing on supporting reads.
rule mean_family_depth:
    input:
        bam = umi_dir + '{sample}.{step}.bam'
    output:
        temp_fl = qc_dir + '{sample}.{step}.temp.fl',
        totalsum = qc_dir + '{sample}.{step}.reads_per_umi',
        meandepth = qc_dir + '{sample}.meandepth'
    params:
        conda = get_conda_env(config["conda_env_yaml"],"samtools"),
        name = '{sample}_{step}'
    singularity: singularity_image
    benchmark:
        benchmark_dir + "{sample}.{step}.meanfamilydepth.tsv"
    threads:
        get_threads(cluster_config, 'mean_family_depth')
    shell:
        "source activate {params.conda}\n"
        "samtools view {input.bam} | grep 'RX:Z:' | sed 's/.*RX:Z:\([ACGT-].*\).*/\\1/' | cut -f1 | grep -v 'N' | sort | uniq -c | sed -e 's/  */\\t/g' | cut -f2,3 > {output.temp_fl}\n"
        "awk -F'\\t' '{{sum+=$1;}} END{{printf(\"{params.name}_meandepth: \"sum/NR)}}' {output.temp_fl} > {output.totalsum}\n"
        "cat {output.totalsum} >> {output.meandepth}"
