# vim: syntax=python tabstop=4 expandtab
# coding: utf-8

## UmiAwareMarkDuplicatesWithMateCigar - umimetrics
rule picard_umiaware:
    input:
        bam = umi_dir + "{sample}.consensusfiltered.umi.bam"
    output:
        bam = umi_qc_dir + "{sample}.picard.umiaware.bam",
        duplicates = umi_qc_dir + "{sample}.umi.duplicatemetrics",
        umimetrics = umi_qc_dir + "{sample}.umi.metrics"
    benchmark:
        Path(benchmark_dir + "picard_umiaware_{sample}.tsv").as_posix()
    singularity: 
        Path(singularity_image, config["bioinfo_tools"].get("picard") + ".sif").as_posix()
    params:
        conda = config["bioinfo_tools"].get("picard"),
        sample_id = "{sample}"
    threads:
        get_threads(cluster_config, "picard_umiaware")
    message:
        "Picard Umiaware mark dups for sample {params.sample_id}"
    shell:
        """
source activate {params.conda};

picard UmiAwareMarkDuplicatesWithMateCigar \
I={input.bam} \
O={output.bam} \
M={output.duplicates} \
UMI_METRICS={output.umimetrics};
        """

## CollectHSmetrics - median target coverage-required
rule picard_collecthsmetrics_umi:
    input:
        fadict = (config["reference"]["reference_genome"]).replace(".fasta",".dict"),
        bed = config["panel"]["capture_kit"],
        bam = umi_dir + "{sample}.consensusfiltered.umi.bam",
        fa = config["reference"]["reference_genome"]
    output:
        mrkdup = umi_qc_dir + "{sample}.umi.collect_hsmetric"
    benchmark:
        Path(benchmark_dir + "picard_collecthsmetrics_umi_{sample}.tsv").as_posix()
    singularity:
        Path(singularity_image, config["bioinfo_tools"].get("picard") + ".sif").as_posix()
    params:
        conda = config["bioinfo_tools"].get("picard"),
        baitsetname = os.path.basename(config["panel"]["capture_kit"]),
        sample_id = "{sample}"
    threads: 
        get_threads(cluster_config, "CollectHsMetrics")
    message:
        "Collect HSmetrics using Picardtools for {params.sample_id}"
    shell:
        """
source activate {params.conda};

picard BedToIntervalList \
I={input.bed} \
O={input.bam}.picard.bedintervals \
SD={input.fadict};

picard CollectHsMetrics \
BI={input.bam}.picard.bedintervals \
TI={input.bam}.picard.bedintervals \
I={input.bam} \
R={input.fa} \
O={output.mrkdup} \
COVERAGE_CAP=50000;
        """

## SUM(Reads in each family)/ the number of families after correction, collapsing on supporting reads.
rule samtools_view_calculatemeanfamilydepth_umi:
    input:
        bam = umi_dir + "{sample}.consensusfiltered.umi.bam"
    output:
        temp_fl = temp (umi_qc_dir + "{sample}.umi.temp.fl"),
        totalsum = umi_qc_dir + "{sample}.umi.mean_family_depth"
    benchmark:
        Path(benchmark_dir + "samtools_view_calculatemeanfamilydepth_umi_{sample}.tsv").as_posix()
    singularity:
        Path(singularity_image, config["bioinfo_tools"].get("samtools") + ".sif").as_posix()
    params:
        conda = config["bioinfo_tools"].get("samtools"),
        sample_id = "{sample}"
    threads:
        get_threads(cluster_config, "samtools_view_calculatemeanfamilydepth_umi")
    message:
        "Calculating mean family depth using samtools and awk for {params.sample_id}"
    shell:
        """
source activate {params.conda};

samtools view -@ {threads} {input.bam} | \
grep 'RX:Z:' | \
sed 's/.*RX:Z:\\([ACGT-].*\\).*/\\1/' | \
cut -f1 | \
grep -v 'N' | \
sort | uniq -c | \
sed -e 's/  */\\t/g' | \
cut -f2,3 > {output.temp_fl};

awk -F'\\t' \
'{{sum+=$1;}} \
END{{printf(\"{params.sample_id}_meandepth: \"sum/NR)}}' \
{output.temp_fl} > \
{output.totalsum}
        """
